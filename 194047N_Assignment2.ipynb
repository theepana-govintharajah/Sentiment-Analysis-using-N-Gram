{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For all explanation questions, please refer pdf document which I have uploaded. Sorry for the inconvinience."
      ],
      "metadata": {
        "id": "oyUfTVypc5p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "jw34G3yUY_lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "from nltk.util import bigrams\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "SPZvdFwh4-h1"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tRy8sT5KaoB",
        "outputId": "0ecea890-a537-4922-a5fe-850da65ce339"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stop words (if not already downloaded)\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BhD-ynr4eBX",
        "outputId": "0d9c10fc-a04a-46a9-9cdd-2d9ac71bf0d3"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7GNWcPN3H4P",
        "outputId": "ca3b8c5c-e2a0-488e-e5ce-4bb132fa5308"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 1: Import the movie reviews"
      ],
      "metadata": {
        "id": "eNAK7Vv54Q6I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "YjDz_wumm6kY"
      },
      "outputs": [],
      "source": [
        "# Open the file for reading\n",
        "with open(\"drive/MyDrive/Colab Notebooks/NLP/Movie_Reviews.txt\", \"r\") as file:\n",
        "    # Initialize variables to store positive and negative reviews\n",
        "    positive_reviews = []\n",
        "    negative_reviews = []\n",
        "\n",
        "    # Initialize a flag to keep track of whether we're reading positive or negative reviews\n",
        "    reading_positive = False\n",
        "    reading_negative = False\n",
        "\n",
        "    # Iterate through the lines in the file\n",
        "    for line in file:\n",
        "        # Remove leading and trailing whitespace\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check if the line indicates positive reviews\n",
        "        if line == \"Positive Reviews\":\n",
        "            reading_positive = True\n",
        "            reading_negative = False\n",
        "            next(file)  # Skip the additional line\n",
        "        # Check if the line indicates negative reviews\n",
        "        elif line == \"Negative Reviews\":\n",
        "            reading_positive = False\n",
        "            reading_negative = True\n",
        "            next(file)  # Skip the additional line\n",
        "        # If we're reading positive reviews, add the review to the positive_reviews list\n",
        "        elif reading_positive and line:\n",
        "            positive_reviews.append(line)\n",
        "        # If we're reading negative reviews, add the review to the negative_reviews list\n",
        "        elif reading_negative and line:\n",
        "            if not line.startswith(\"===\"):\n",
        "                negative_reviews.append(line)\n",
        "            else:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the extracted reviews\n",
        "print(\"Positive Reviews:\")\n",
        "for review in positive_reviews:\n",
        "    print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c6rhFKt5q-u",
        "outputId": "02ccad7e-f0ee-453a-8b51-7c14985b4af2"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Reviews:\n",
            "1. \"Forrest Gump is an absolute masterpiece! Tom Hanks delivers an unforgettable performance, and the storytelling is heartwarming. This movie is a journey through life that will make you laugh, cry, and appreciate the simple beauties of existence.\"\n",
            "2. \"The Shawshank Redemption is a timeless classic. The powerful themes of hope, friendship, and redemption make it a must-watch. Morgan Freeman and Tim Robbins give exceptional performances in this brilliantly crafted film.\"\n",
            "3. \"The epic conclusion to The Lord of the Rings trilogy, The Return of the King, is a cinematic triumph. The breathtaking visuals, epic battles, and emotionally resonant story make it a monumental achievement in filmmaking.\"\n",
            "4. \"La La Land is a love letter to the magic of Hollywood and dreams. The chemistry between Ryan Gosling and Emma Stone is enchanting, and the music and dance sequences are a pure delight. A modern musical masterpiece.\"\n",
            "5. \"Wes Anderson's whimsical style shines in The Grand Budapest Hotel. With its quirky characters and colorful cinematography, it's a visual and narrative delight. This film is a charming and delightful experience.\"\n",
            "6.\"Inception is mind-bending brilliance! Christopher Nolan's intricate plot, stunning visual effects, and Hans Zimmer's haunting score create a cinematic journey that keeps you on the edge of your seat. A true masterpiece of sci-fi cinema.\"\n",
            "7. \"The Social Network is a captivating exploration of the creation of Facebook and the personal and legal conflicts that ensued. David Fincher's direction and Aaron Sorkin's sharp screenplay make this film a modern classic.\"\n",
            "8. \"Will Smith's portrayal of Chris Gardner in The Pursuit of Happyness is both touching and inspirational. This film reminds us that with determination and unwavering spirit, anyone can overcome adversity to achieve their dreams.\"\n",
            "9. \"Eternal Sunshine of the Spotless Mind is a beautifully unconventional love story. Jim Carrey and Kate Winslet shine in their roles, and the narrative, told in a non-linear fashion, is a poignant exploration of love, memories, and human connection.\"\n",
            "10. \"The Princess Bride is a timeless fairy tale with a perfect blend of humor, romance, and adventure. Its witty dialogue and memorable characters make it a film that appeals to both kids and adults. Inconceivably delightful!\"\n",
            "11. \"Fifty Shades of Grey\" has managed to captivate both enthusiasts and critics alike. The film's ability to spark passionate discussions and elicit a wide range of opinions is a testament to its impact. While some may find it controversial, there's no denying that it has left a significant mark on the world of cinema.\n",
            "12. \"This film has been a conversation starter, and it's clear that it won't be everyone's cup of tea. But for those willing to approach it with an open mind, there's a visually lush experience with moments of genuine chemistry between the leads. It's a film that has managed to offer something different and intriguing in the world of romance and drama.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the lengths of positive and negative reviews\n",
        "print(f\"Number of Positive Reviews: {len(positive_reviews)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDDinoQi55vT",
        "outputId": "1120b1e4-acd8-45d6-b698-35aa0aec3023"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Positive Reviews: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNegative Reviews:\")\n",
        "for review in negative_reviews:\n",
        "    print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVvX6OkE7VBc",
        "outputId": "91016ae6-e5e2-4252-b7d2-b3d236e0f2ab"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Negative Reviews:\n",
            "1. \"The Last Airbender is a disaster of a film adaptation. It butchers the beloved animated series with wooden acting, convoluted storytelling, and cringe-worthy special effects. A letdown for fans and newcomers alike.\"\n",
            "2. \"Another Transformers movie, and it's just more of the same: mindless explosions, incoherent plotlines, and an overreliance on CGI. This franchise desperately needs an overhaul.\"\n",
            "3. \"The Emoji Movie is a blatant cash grab with a shallow, uninspired plot. It fails to deliver clever humor or meaningful messages, making it a forgettable and disappointing animated film.\"\n",
            "4. \"Fifty Shades of Grey is a cringe-inducing attempt at romance. Poorly written dialogue and unconvincing chemistry between the leads make it an awkward and unfulfilling cinematic experience.\"\n",
            "5. \"Jack and Jill is an unbearable comedy that relies on stale humor and a painfully unfunny portrayal of Adam Sandler in a dual role. It's a prime example of lazy filmmaking.\"\n",
            "6. \"Superman IV is a colossal disappointment. It's marred by a low budget, laughable special effects, and a poorly conceived story. Even Christopher Reeve's charm can't save this mess.\"\n",
            "7. \"The Cat in the Hat is a chaotic and misguided adaptation of Dr. Seuss's classic. It sacrifices the charm and simplicity of the source material for crude humor and a lackluster narrative.\"\n",
            "8. \"The Room is widely regarded as one of the worst films ever made. Its disjointed plot, stilted acting, and bizarre dialogue have turned it into a cult classic, but not for the right reasons.\"\n",
            "9. \"Battlefield Earth is a sci-fi disaster. It's a convoluted mess with hammy acting and laughable special effects. A film that should have remained buried in the annals of cinematic history.\"\n",
            "10. \"Gigli is a train wreck of a romantic comedy. The pairing of Ben Affleck and Jennifer Lopez is devoid of chemistry, and the dialogue is cringeworthy. It's an embarrassing misstep in both their careers.\"\n",
            "11. \"Fifty Shades of Grey\" is a film that has divided enthusiasts and critics. While it has its share of devoted fans, it also faces substantial criticism. For some, the film's content and execution leave much to be desired, making it a polarizing cinematic experience. Viewer reactions may vary widely, so it's a movie that evokes strong opinions on both ends of the spectrum.\n",
            "12. \"This film's portrayal of relationships and themes may leave many viewers uncomfortable. It tests the limits of what some may find acceptable in mainstream cinema. So, while the idea of keeping an open mind is essential in film-watching, this particular movie might push that openness to its boundaries for some.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the lengths of positive and negative reviews\n",
        "print(f\"Number of Negative Reviews: {len(negative_reviews)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOhZDWPF87bL",
        "outputId": "e781d123-a75d-443f-eb21-94c3ea43449f"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Negative Reviews: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 2: Pre-processing"
      ],
      "metadata": {
        "id": "r_Etoe5b4Xcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "# Instantiate the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "xef_vNL--tKs"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess a text\n",
        "def preprocess_text(text):\n",
        "    # Tokenization: Split the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    words = [word.strip(string.punctuation).lower() for word in words]\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Perform stemming\n",
        "    #words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Perform lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Remove empty strings\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    # Check if the first word is a numeric character and remove it, because there are leading numbers like 1,2,3,...\n",
        "    if words and words[0].isdigit():\n",
        "        words = words[1:]\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "3hUbifpi-3K6"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the positive and negative reviews\n",
        "preprocessed_positive_reviews = [preprocess_text(review) for review in positive_reviews]\n",
        "preprocessed_negative_reviews = [preprocess_text(review) for review in negative_reviews]"
      ],
      "metadata": {
        "id": "Strd8HBk-69J"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the preprocessed reviews\n",
        "print(\"Preprocessed Positive Reviews:\")\n",
        "for review in preprocessed_positive_reviews:\n",
        "    print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDygHyIQ-_NT",
        "outputId": "a73221e9-8f88-4aef-e55b-706d6ce5df05"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Positive Reviews:\n",
            "['forrest', 'gump', 'absolute', 'masterpiece', 'tom', 'hank', 'delivers', 'unforgettable', 'performance', 'storytelling', 'heartwarming', 'movie', 'journey', 'life', 'make', 'laugh', 'cry', 'appreciate', 'simple', 'beauty', 'existence']\n",
            "['shawshank', 'redemption', 'timeless', 'classic', 'powerful', 'theme', 'hope', 'friendship', 'redemption', 'make', 'must-watch', 'morgan', 'freeman', 'tim', 'robbins', 'give', 'exceptional', 'performance', 'brilliantly', 'crafted', 'film']\n",
            "['epic', 'conclusion', 'lord', 'ring', 'trilogy', 'return', 'king', 'cinematic', 'triumph', 'breathtaking', 'visuals', 'epic', 'battle', 'emotionally', 'resonant', 'story', 'make', 'monumental', 'achievement', 'filmmaking']\n",
            "['la', 'la', 'land', 'love', 'letter', 'magic', 'hollywood', 'dream', 'chemistry', 'ryan', 'gosling', 'emma', 'stone', 'enchanting', 'music', 'dance', 'sequence', 'pure', 'delight', 'modern', 'musical', 'masterpiece']\n",
            "['wes', 'anderson', 'whimsical', 'style', 'shine', 'grand', 'budapest', 'hotel', 'quirky', 'character', 'colorful', 'cinematography', 'visual', 'narrative', 'delight', 'film', 'charming', 'delightful', 'experience']\n",
            "['inception', 'mind-bending', 'brilliance', 'christopher', 'nolan', 'intricate', 'plot', 'stunning', 'visual', 'effect', 'han', 'zimmer', 'haunting', 'score', 'create', 'cinematic', 'journey', 'keep', 'edge', 'seat', 'true', 'masterpiece', 'sci-fi', 'cinema']\n",
            "['social', 'network', 'captivating', 'exploration', 'creation', 'facebook', 'personal', 'legal', 'conflict', 'ensued', 'david', 'fincher', 'direction', 'aaron', 'sorkin', 'sharp', 'screenplay', 'make', 'film', 'modern', 'classic']\n",
            "['smith', 'portrayal', 'chris', 'gardner', 'pursuit', 'happyness', 'touching', 'inspirational', 'film', 'reminds', 'u', 'determination', 'unwavering', 'spirit', 'anyone', 'overcome', 'adversity', 'achieve', 'dream']\n",
            "['eternal', 'sunshine', 'spotless', 'mind', 'beautifully', 'unconventional', 'love', 'story', 'jim', 'carrey', 'kate', 'winslet', 'shine', 'role', 'narrative', 'told', 'non-linear', 'fashion', 'poignant', 'exploration', 'love', 'memory', 'human', 'connection']\n",
            "['princess', 'bride', 'timeless', 'fairy', 'tale', 'perfect', 'blend', 'humor', 'romance', 'adventure', 'witty', 'dialogue', 'memorable', 'character', 'make', 'film', 'appeal', 'kid', 'adult', 'inconceivably', 'delightful']\n",
            "['fifty', 'shade', 'grey', 'managed', 'captivate', 'enthusiast', 'critic', 'alike', 'film', 'ability', 'spark', 'passionate', 'discussion', 'elicit', 'wide', 'range', 'opinion', 'testament', 'impact', 'may', 'find', 'controversial', 'denying', 'left', 'significant', 'mark', 'world', 'cinema']\n",
            "['film', 'conversation', 'starter', 'clear', 'wo', \"n't\", 'everyone', 'cup', 'tea', 'willing', 'approach', 'open', 'mind', 'visually', 'lush', 'experience', 'moment', 'genuine', 'chemistry', 'lead', 'film', 'managed', 'offer', 'something', 'different', 'intriguing', 'world', 'romance', 'drama']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test movie review\n",
        "test_review = \"It's clear that the movie has both its enthusiasts and critics. While it may not be to everyone's taste, it's worth watching with an open mind to form your own opinion.\""
      ],
      "metadata": {
        "id": "Nvi8a3UPTTfG"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test review using the same preprocessing function used for training data\n",
        "preprocessed_test_review = preprocess_text(test_review)\n",
        "preprocessed_test_review"
      ],
      "metadata": {
        "id": "n1U4f6mxT_vX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7602a9-f358-4c8f-dd7e-2eac3f0f7533"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['clear',\n",
              " 'movie',\n",
              " 'enthusiast',\n",
              " 'critic',\n",
              " 'may',\n",
              " 'everyone',\n",
              " 'taste',\n",
              " 'worth',\n",
              " 'watching',\n",
              " 'open',\n",
              " 'mind',\n",
              " 'form',\n",
              " 'opinion']"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the vocabulary size for Laplace smoothing\n",
        "vocabulary_size_positive=len(set(word for review in preprocessed_positive_reviews for word in review))\n",
        "vocabulary_size_positive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1O6vQzYAVV9",
        "outputId": "9b05425b-4844-4849-ad4e-946aeb428451"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "229"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the vocabulary size for Laplace smoothing\n",
        "vocabulary_size_negative=len(set(word for review in preprocessed_negative_reviews for word in review))\n",
        "vocabulary_size_negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_juPNjLAe2n",
        "outputId": "628d390c-9024-44e1-cefd-22a4a49b5faa"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 3: Choosing the correct N-Gram model"
      ],
      "metadata": {
        "id": "WicA5uktACp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a small dataset, which contains only 12 positive and negative reviews, we can't go for high n values. Because larger N-grams require more data to estimate probabilities accurately. And also this is a sentiment analysis task, its better to use high N value for word prediction like task, because for those it can able to capture context and long dependencies. But for sentiment analysis small value of N is enough. And also due to small dataset we can't go for N>=3, it's better to go with unigram or bigram. (Additionally for this small dataset, if we go for trigram, probability of all words become 0 for this particular test review )\n"
      ],
      "metadata": {
        "id": "thexGCXlQVrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eventhough I have done the calculation for both Uni-Gram and Bi-Gram -> Most suitable model is Bi-Gram model (N=2)"
      ],
      "metadata": {
        "id": "jX-VVum3tVpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram"
      ],
      "metadata": {
        "id": "vWMpzlNPQhZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 3: Implementation of Bi-Gram model"
      ],
      "metadata": {
        "id": "vB2RZuF4WNj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bigram model\n",
        "def build_bigram_model(reviews):\n",
        "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "    unigram_counts = defaultdict(int)\n",
        "\n",
        "    for review in reviews:\n",
        "       # Add <S> at the beginning and </S> at the end of each review\n",
        "        review = ['<S>'] + review + ['</S>']\n",
        "\n",
        "        for i in range(len(review) - 1):\n",
        "            bigram = (review[i], review[i + 1])\n",
        "            bigram_counts[bigram[0]][bigram[1]] += 1\n",
        "            unigram_counts[bigram[0]] += 1\n",
        "\n",
        "    # Calculate bigram probabilities\n",
        "    bigram_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "    bigram_count = defaultdict(lambda: defaultdict(int))\n",
        "    for word1, next_words in bigram_counts.items():\n",
        "        for word2, count in next_words.items():\n",
        "            bigram_count[word1][word2] = count\n",
        "            bigram_probabilities[word1][word2] = count / unigram_counts[word1]\n",
        "\n",
        "    return bigram_probabilities, bigram_count"
      ],
      "metadata": {
        "id": "EBitV89jolje"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bigram models for the positive and negative train sets\n",
        "positive_train_bigram_model, positive_bigram_count = build_bigram_model(preprocessed_positive_reviews)\n",
        "negative_train_bigram_model, negative_bigram_count = build_bigram_model(preprocessed_negative_reviews)"
      ],
      "metadata": {
        "id": "BuCpPLnVDX05"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Flatten the list of lists to get unique words\n",
        "unique_words_positive = list(set(word for review in preprocessed_positive_reviews for word in review))\n",
        "\n",
        "# Create a DataFrame for the bigram count matrix for the positive review\n",
        "bigram_count_data = {word1: [positive_bigram_count[word1][word2] for word2 in unique_words_positive] for word1 in unique_words_positive}\n",
        "bigram_count_df = pd.DataFrame(bigram_count_data, index=unique_words_positive)\n",
        "\n",
        "# Print the bigram count matrix for the positive review\n",
        "print(\"Bigram Count Matrix (Positive Review):\")\n",
        "bigram_count_df\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "xmiPuJvBVKTM",
        "outputId": "91487857-090d-4c1d-d142-0365c39bdece"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Count Matrix (Positive Review):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           ability  hope  morgan  network  music  brilliance  resonant  drama  \\\n",
              "ability          0     0       0        0      0           0         0      0   \n",
              "hope             0     0       0        0      0           0         0      0   \n",
              "morgan           0     0       0        0      0           0         0      0   \n",
              "network          0     0       0        0      0           0         0      0   \n",
              "music            0     0       0        0      0           0         0      0   \n",
              "...            ...   ...     ...      ...    ...         ...       ...    ...   \n",
              "robbins          0     0       0        0      0           0         0      0   \n",
              "overcome         0     0       0        0      0           0         0      0   \n",
              "dream            0     0       0        0      0           0         0      0   \n",
              "kid              0     0       0        0      0           0         0      0   \n",
              "whimsical        0     0       0        0      0           0         0      0   \n",
              "\n",
              "           impact  charming  ...  visuals  keep  pursuit  laugh  timeless  \\\n",
              "ability         0         0  ...        0     0        0      0         0   \n",
              "hope            0         0  ...        0     0        0      0         0   \n",
              "morgan          0         0  ...        0     0        0      0         0   \n",
              "network         0         0  ...        0     0        0      0         0   \n",
              "music           0         0  ...        0     0        0      0         0   \n",
              "...           ...       ...  ...      ...   ...      ...    ...       ...   \n",
              "robbins         0         0  ...        0     0        0      0         0   \n",
              "overcome        0         0  ...        0     0        0      0         0   \n",
              "dream           0         0  ...        0     0        0      0         0   \n",
              "kid             0         0  ...        0     0        0      0         0   \n",
              "whimsical       0         0  ...        0     0        0      0         0   \n",
              "\n",
              "           robbins  overcome  dream  kid  whimsical  \n",
              "ability          0         0      0    0          0  \n",
              "hope             0         0      0    0          0  \n",
              "morgan           0         0      0    0          0  \n",
              "network          0         0      0    0          0  \n",
              "music            0         0      0    0          0  \n",
              "...            ...       ...    ...  ...        ...  \n",
              "robbins          0         0      0    0          0  \n",
              "overcome         0         0      0    0          0  \n",
              "dream            0         0      0    0          0  \n",
              "kid              0         0      0    0          0  \n",
              "whimsical        0         0      0    0          0  \n",
              "\n",
              "[229 rows x 229 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4ab17cb-59ea-4683-a208-89732d325856\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>hope</th>\n",
              "      <th>morgan</th>\n",
              "      <th>network</th>\n",
              "      <th>music</th>\n",
              "      <th>brilliance</th>\n",
              "      <th>resonant</th>\n",
              "      <th>drama</th>\n",
              "      <th>impact</th>\n",
              "      <th>charming</th>\n",
              "      <th>...</th>\n",
              "      <th>visuals</th>\n",
              "      <th>keep</th>\n",
              "      <th>pursuit</th>\n",
              "      <th>laugh</th>\n",
              "      <th>timeless</th>\n",
              "      <th>robbins</th>\n",
              "      <th>overcome</th>\n",
              "      <th>dream</th>\n",
              "      <th>kid</th>\n",
              "      <th>whimsical</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ability</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hope</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>morgan</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>network</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>music</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>robbins</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>overcome</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dream</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kid</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>whimsical</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>229 rows Ã— 229 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4ab17cb-59ea-4683-a208-89732d325856')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4ab17cb-59ea-4683-a208-89732d325856 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4ab17cb-59ea-4683-a208-89732d325856');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba44c761-0b77-42a5-bff6-9e581138f354\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba44c761-0b77-42a5-bff6-9e581138f354')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba44c761-0b77-42a5-bff6-9e581138f354 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 4: Bi-Gram probability"
      ],
      "metadata": {
        "id": "t2h-fBcWF2lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bigram Probabilities for Positive Train Set:\")\n",
        "# Print the bigram probabilities for positive reviews\n",
        "for word1, next_words in positive_train_bigram_model.items():\n",
        "    for word2, probability in next_words.items():\n",
        "        print(f'P({word2} | {word1}) : {probability}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkplYh1KqjbF",
        "outputId": "ae84f2d3-5973-4c67-b713-b2475fec65d8"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probabilities for Positive Train Set:\n",
            "P(forrest | <S>) : 0.08333333333333333\n",
            "P(shawshank | <S>) : 0.08333333333333333\n",
            "P(epic | <S>) : 0.08333333333333333\n",
            "P(la | <S>) : 0.08333333333333333\n",
            "P(wes | <S>) : 0.08333333333333333\n",
            "P(inception | <S>) : 0.08333333333333333\n",
            "P(social | <S>) : 0.08333333333333333\n",
            "P(smith | <S>) : 0.08333333333333333\n",
            "P(eternal | <S>) : 0.08333333333333333\n",
            "P(princess | <S>) : 0.08333333333333333\n",
            "P(fifty | <S>) : 0.08333333333333333\n",
            "P(film | <S>) : 0.08333333333333333\n",
            "P(gump | forrest) : 1.0\n",
            "P(absolute | gump) : 1.0\n",
            "P(masterpiece | absolute) : 1.0\n",
            "P(tom | masterpiece) : 0.3333333333333333\n",
            "P(</S> | masterpiece) : 0.3333333333333333\n",
            "P(sci-fi | masterpiece) : 0.3333333333333333\n",
            "P(hank | tom) : 1.0\n",
            "P(delivers | hank) : 1.0\n",
            "P(unforgettable | delivers) : 1.0\n",
            "P(performance | unforgettable) : 1.0\n",
            "P(storytelling | performance) : 0.5\n",
            "P(brilliantly | performance) : 0.5\n",
            "P(heartwarming | storytelling) : 1.0\n",
            "P(movie | heartwarming) : 1.0\n",
            "P(journey | movie) : 1.0\n",
            "P(life | journey) : 0.5\n",
            "P(keep | journey) : 0.5\n",
            "P(make | life) : 1.0\n",
            "P(laugh | make) : 0.2\n",
            "P(must-watch | make) : 0.2\n",
            "P(monumental | make) : 0.2\n",
            "P(film | make) : 0.4\n",
            "P(cry | laugh) : 1.0\n",
            "P(appreciate | cry) : 1.0\n",
            "P(simple | appreciate) : 1.0\n",
            "P(beauty | simple) : 1.0\n",
            "P(existence | beauty) : 1.0\n",
            "P(</S> | existence) : 1.0\n",
            "P(redemption | shawshank) : 1.0\n",
            "P(timeless | redemption) : 0.5\n",
            "P(make | redemption) : 0.5\n",
            "P(classic | timeless) : 0.5\n",
            "P(fairy | timeless) : 0.5\n",
            "P(powerful | classic) : 0.5\n",
            "P(</S> | classic) : 0.5\n",
            "P(theme | powerful) : 1.0\n",
            "P(hope | theme) : 1.0\n",
            "P(friendship | hope) : 1.0\n",
            "P(redemption | friendship) : 1.0\n",
            "P(morgan | must-watch) : 1.0\n",
            "P(freeman | morgan) : 1.0\n",
            "P(tim | freeman) : 1.0\n",
            "P(robbins | tim) : 1.0\n",
            "P(give | robbins) : 1.0\n",
            "P(exceptional | give) : 1.0\n",
            "P(performance | exceptional) : 1.0\n",
            "P(crafted | brilliantly) : 1.0\n",
            "P(film | crafted) : 1.0\n",
            "P(</S> | film) : 0.125\n",
            "P(charming | film) : 0.125\n",
            "P(modern | film) : 0.125\n",
            "P(reminds | film) : 0.125\n",
            "P(appeal | film) : 0.125\n",
            "P(ability | film) : 0.125\n",
            "P(conversation | film) : 0.125\n",
            "P(managed | film) : 0.125\n",
            "P(conclusion | epic) : 0.5\n",
            "P(battle | epic) : 0.5\n",
            "P(lord | conclusion) : 1.0\n",
            "P(ring | lord) : 1.0\n",
            "P(trilogy | ring) : 1.0\n",
            "P(return | trilogy) : 1.0\n",
            "P(king | return) : 1.0\n",
            "P(cinematic | king) : 1.0\n",
            "P(triumph | cinematic) : 0.5\n",
            "P(journey | cinematic) : 0.5\n",
            "P(breathtaking | triumph) : 1.0\n",
            "P(visuals | breathtaking) : 1.0\n",
            "P(epic | visuals) : 1.0\n",
            "P(emotionally | battle) : 1.0\n",
            "P(resonant | emotionally) : 1.0\n",
            "P(story | resonant) : 1.0\n",
            "P(make | story) : 0.5\n",
            "P(jim | story) : 0.5\n",
            "P(achievement | monumental) : 1.0\n",
            "P(filmmaking | achievement) : 1.0\n",
            "P(</S> | filmmaking) : 1.0\n",
            "P(la | la) : 0.5\n",
            "P(land | la) : 0.5\n",
            "P(love | land) : 1.0\n",
            "P(letter | love) : 0.3333333333333333\n",
            "P(story | love) : 0.3333333333333333\n",
            "P(memory | love) : 0.3333333333333333\n",
            "P(magic | letter) : 1.0\n",
            "P(hollywood | magic) : 1.0\n",
            "P(dream | hollywood) : 1.0\n",
            "P(chemistry | dream) : 0.5\n",
            "P(</S> | dream) : 0.5\n",
            "P(ryan | chemistry) : 0.5\n",
            "P(lead | chemistry) : 0.5\n",
            "P(gosling | ryan) : 1.0\n",
            "P(emma | gosling) : 1.0\n",
            "P(stone | emma) : 1.0\n",
            "P(enchanting | stone) : 1.0\n",
            "P(music | enchanting) : 1.0\n",
            "P(dance | music) : 1.0\n",
            "P(sequence | dance) : 1.0\n",
            "P(pure | sequence) : 1.0\n",
            "P(delight | pure) : 1.0\n",
            "P(modern | delight) : 0.5\n",
            "P(film | delight) : 0.5\n",
            "P(musical | modern) : 0.5\n",
            "P(classic | modern) : 0.5\n",
            "P(masterpiece | musical) : 1.0\n",
            "P(anderson | wes) : 1.0\n",
            "P(whimsical | anderson) : 1.0\n",
            "P(style | whimsical) : 1.0\n",
            "P(shine | style) : 1.0\n",
            "P(grand | shine) : 0.5\n",
            "P(role | shine) : 0.5\n",
            "P(budapest | grand) : 1.0\n",
            "P(hotel | budapest) : 1.0\n",
            "P(quirky | hotel) : 1.0\n",
            "P(character | quirky) : 1.0\n",
            "P(colorful | character) : 0.5\n",
            "P(make | character) : 0.5\n",
            "P(cinematography | colorful) : 1.0\n",
            "P(visual | cinematography) : 1.0\n",
            "P(narrative | visual) : 0.5\n",
            "P(effect | visual) : 0.5\n",
            "P(delight | narrative) : 0.5\n",
            "P(told | narrative) : 0.5\n",
            "P(delightful | charming) : 1.0\n",
            "P(experience | delightful) : 0.5\n",
            "P(</S> | delightful) : 0.5\n",
            "P(</S> | experience) : 0.5\n",
            "P(moment | experience) : 0.5\n",
            "P(mind-bending | inception) : 1.0\n",
            "P(brilliance | mind-bending) : 1.0\n",
            "P(christopher | brilliance) : 1.0\n",
            "P(nolan | christopher) : 1.0\n",
            "P(intricate | nolan) : 1.0\n",
            "P(plot | intricate) : 1.0\n",
            "P(stunning | plot) : 1.0\n",
            "P(visual | stunning) : 1.0\n",
            "P(han | effect) : 1.0\n",
            "P(zimmer | han) : 1.0\n",
            "P(haunting | zimmer) : 1.0\n",
            "P(score | haunting) : 1.0\n",
            "P(create | score) : 1.0\n",
            "P(cinematic | create) : 1.0\n",
            "P(edge | keep) : 1.0\n",
            "P(seat | edge) : 1.0\n",
            "P(true | seat) : 1.0\n",
            "P(masterpiece | true) : 1.0\n",
            "P(cinema | sci-fi) : 1.0\n",
            "P(</S> | cinema) : 1.0\n",
            "P(network | social) : 1.0\n",
            "P(captivating | network) : 1.0\n",
            "P(exploration | captivating) : 1.0\n",
            "P(creation | exploration) : 0.5\n",
            "P(love | exploration) : 0.5\n",
            "P(facebook | creation) : 1.0\n",
            "P(personal | facebook) : 1.0\n",
            "P(legal | personal) : 1.0\n",
            "P(conflict | legal) : 1.0\n",
            "P(ensued | conflict) : 1.0\n",
            "P(david | ensued) : 1.0\n",
            "P(fincher | david) : 1.0\n",
            "P(direction | fincher) : 1.0\n",
            "P(aaron | direction) : 1.0\n",
            "P(sorkin | aaron) : 1.0\n",
            "P(sharp | sorkin) : 1.0\n",
            "P(screenplay | sharp) : 1.0\n",
            "P(make | screenplay) : 1.0\n",
            "P(portrayal | smith) : 1.0\n",
            "P(chris | portrayal) : 1.0\n",
            "P(gardner | chris) : 1.0\n",
            "P(pursuit | gardner) : 1.0\n",
            "P(happyness | pursuit) : 1.0\n",
            "P(touching | happyness) : 1.0\n",
            "P(inspirational | touching) : 1.0\n",
            "P(film | inspirational) : 1.0\n",
            "P(u | reminds) : 1.0\n",
            "P(determination | u) : 1.0\n",
            "P(unwavering | determination) : 1.0\n",
            "P(spirit | unwavering) : 1.0\n",
            "P(anyone | spirit) : 1.0\n",
            "P(overcome | anyone) : 1.0\n",
            "P(adversity | overcome) : 1.0\n",
            "P(achieve | adversity) : 1.0\n",
            "P(dream | achieve) : 1.0\n",
            "P(sunshine | eternal) : 1.0\n",
            "P(spotless | sunshine) : 1.0\n",
            "P(mind | spotless) : 1.0\n",
            "P(beautifully | mind) : 0.5\n",
            "P(visually | mind) : 0.5\n",
            "P(unconventional | beautifully) : 1.0\n",
            "P(love | unconventional) : 1.0\n",
            "P(carrey | jim) : 1.0\n",
            "P(kate | carrey) : 1.0\n",
            "P(winslet | kate) : 1.0\n",
            "P(shine | winslet) : 1.0\n",
            "P(narrative | role) : 1.0\n",
            "P(non-linear | told) : 1.0\n",
            "P(fashion | non-linear) : 1.0\n",
            "P(poignant | fashion) : 1.0\n",
            "P(exploration | poignant) : 1.0\n",
            "P(human | memory) : 1.0\n",
            "P(connection | human) : 1.0\n",
            "P(</S> | connection) : 1.0\n",
            "P(bride | princess) : 1.0\n",
            "P(timeless | bride) : 1.0\n",
            "P(tale | fairy) : 1.0\n",
            "P(perfect | tale) : 1.0\n",
            "P(blend | perfect) : 1.0\n",
            "P(humor | blend) : 1.0\n",
            "P(romance | humor) : 1.0\n",
            "P(adventure | romance) : 0.5\n",
            "P(drama | romance) : 0.5\n",
            "P(witty | adventure) : 1.0\n",
            "P(dialogue | witty) : 1.0\n",
            "P(memorable | dialogue) : 1.0\n",
            "P(character | memorable) : 1.0\n",
            "P(kid | appeal) : 1.0\n",
            "P(adult | kid) : 1.0\n",
            "P(inconceivably | adult) : 1.0\n",
            "P(delightful | inconceivably) : 1.0\n",
            "P(shade | fifty) : 1.0\n",
            "P(grey | shade) : 1.0\n",
            "P(managed | grey) : 1.0\n",
            "P(captivate | managed) : 0.5\n",
            "P(offer | managed) : 0.5\n",
            "P(enthusiast | captivate) : 1.0\n",
            "P(critic | enthusiast) : 1.0\n",
            "P(alike | critic) : 1.0\n",
            "P(film | alike) : 1.0\n",
            "P(spark | ability) : 1.0\n",
            "P(passionate | spark) : 1.0\n",
            "P(discussion | passionate) : 1.0\n",
            "P(elicit | discussion) : 1.0\n",
            "P(wide | elicit) : 1.0\n",
            "P(range | wide) : 1.0\n",
            "P(opinion | range) : 1.0\n",
            "P(testament | opinion) : 1.0\n",
            "P(impact | testament) : 1.0\n",
            "P(may | impact) : 1.0\n",
            "P(find | may) : 1.0\n",
            "P(controversial | find) : 1.0\n",
            "P(denying | controversial) : 1.0\n",
            "P(left | denying) : 1.0\n",
            "P(significant | left) : 1.0\n",
            "P(mark | significant) : 1.0\n",
            "P(world | mark) : 1.0\n",
            "P(cinema | world) : 0.5\n",
            "P(romance | world) : 0.5\n",
            "P(starter | conversation) : 1.0\n",
            "P(clear | starter) : 1.0\n",
            "P(wo | clear) : 1.0\n",
            "P(n't | wo) : 1.0\n",
            "P(everyone | n't) : 1.0\n",
            "P(cup | everyone) : 1.0\n",
            "P(tea | cup) : 1.0\n",
            "P(willing | tea) : 1.0\n",
            "P(approach | willing) : 1.0\n",
            "P(open | approach) : 1.0\n",
            "P(mind | open) : 1.0\n",
            "P(lush | visually) : 1.0\n",
            "P(experience | lush) : 1.0\n",
            "P(genuine | moment) : 1.0\n",
            "P(chemistry | genuine) : 1.0\n",
            "P(film | lead) : 1.0\n",
            "P(something | offer) : 1.0\n",
            "P(different | something) : 1.0\n",
            "P(intriguing | different) : 1.0\n",
            "P(world | intriguing) : 1.0\n",
            "P(</S> | drama) : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bigram Probabilities for Negative Train Set:\")\n",
        "for word1, next_words in negative_train_bigram_model.items():\n",
        "    for word2, probability in next_words.items():\n",
        "        print(f'P({word2} | {word1}) : {probability}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn3x3na2S0Q-",
        "outputId": "eab2f2e8-c1c0-4a19-a8a5-e908b4b1c733"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probabilities for Negative Train Set:\n",
            "P(last | <S>) : 0.08333333333333333\n",
            "P(another | <S>) : 0.08333333333333333\n",
            "P(emoji | <S>) : 0.08333333333333333\n",
            "P(fifty | <S>) : 0.16666666666666666\n",
            "P(jack | <S>) : 0.08333333333333333\n",
            "P(superman | <S>) : 0.08333333333333333\n",
            "P(cat | <S>) : 0.08333333333333333\n",
            "P(room | <S>) : 0.08333333333333333\n",
            "P(battlefield | <S>) : 0.08333333333333333\n",
            "P(gigli | <S>) : 0.08333333333333333\n",
            "P(film | <S>) : 0.08333333333333333\n",
            "P(airbender | last) : 1.0\n",
            "P(disaster | airbender) : 1.0\n",
            "P(film | disaster) : 0.5\n",
            "P(convoluted | disaster) : 0.5\n",
            "P(adaptation | film) : 0.14285714285714285\n",
            "P(</S> | film) : 0.14285714285714285\n",
            "P(ever | film) : 0.14285714285714285\n",
            "P(remained | film) : 0.14285714285714285\n",
            "P(divided | film) : 0.14285714285714285\n",
            "P(content | film) : 0.14285714285714285\n",
            "P(portrayal | film) : 0.14285714285714285\n",
            "P(butcher | adaptation) : 0.5\n",
            "P(dr | adaptation) : 0.5\n",
            "P(beloved | butcher) : 1.0\n",
            "P(animated | beloved) : 1.0\n",
            "P(series | animated) : 0.5\n",
            "P(film | animated) : 0.5\n",
            "P(wooden | series) : 1.0\n",
            "P(acting | wooden) : 1.0\n",
            "P(convoluted | acting) : 0.3333333333333333\n",
            "P(bizarre | acting) : 0.3333333333333333\n",
            "P(laughable | acting) : 0.3333333333333333\n",
            "P(storytelling | convoluted) : 0.5\n",
            "P(mess | convoluted) : 0.5\n",
            "P(cringe-worthy | storytelling) : 1.0\n",
            "P(special | cringe-worthy) : 1.0\n",
            "P(effect | special) : 1.0\n",
            "P(letdown | effect) : 0.3333333333333333\n",
            "P(poorly | effect) : 0.3333333333333333\n",
            "P(film | effect) : 0.3333333333333333\n",
            "P(fan | letdown) : 1.0\n",
            "P(newcomer | fan) : 0.5\n",
            "P(also | fan) : 0.5\n",
            "P(alike | newcomer) : 1.0\n",
            "P(</S> | alike) : 1.0\n",
            "P(transformer | another) : 1.0\n",
            "P(movie | transformer) : 1.0\n",
            "P(mindless | movie) : 0.25\n",
            "P(blatant | movie) : 0.25\n",
            "P(evokes | movie) : 0.25\n",
            "P(might | movie) : 0.25\n",
            "P(explosion | mindless) : 1.0\n",
            "P(incoherent | explosion) : 1.0\n",
            "P(plotlines | incoherent) : 1.0\n",
            "P(overreliance | plotlines) : 1.0\n",
            "P(cgi | overreliance) : 1.0\n",
            "P(franchise | cgi) : 1.0\n",
            "P(desperately | franchise) : 1.0\n",
            "P(need | desperately) : 1.0\n",
            "P(overhaul | need) : 1.0\n",
            "P(</S> | overhaul) : 1.0\n",
            "P(movie | emoji) : 1.0\n",
            "P(cash | blatant) : 1.0\n",
            "P(grab | cash) : 1.0\n",
            "P(shallow | grab) : 1.0\n",
            "P(uninspired | shallow) : 1.0\n",
            "P(plot | uninspired) : 1.0\n",
            "P(fails | plot) : 0.5\n",
            "P(stilted | plot) : 0.5\n",
            "P(deliver | fails) : 1.0\n",
            "P(clever | deliver) : 1.0\n",
            "P(humor | clever) : 1.0\n",
            "P(meaningful | humor) : 0.3333333333333333\n",
            "P(painfully | humor) : 0.3333333333333333\n",
            "P(lackluster | humor) : 0.3333333333333333\n",
            "P(message | meaningful) : 1.0\n",
            "P(making | message) : 1.0\n",
            "P(forgettable | making) : 0.5\n",
            "P(polarizing | making) : 0.5\n",
            "P(disappointing | forgettable) : 1.0\n",
            "P(animated | disappointing) : 1.0\n",
            "P(shade | fifty) : 1.0\n",
            "P(grey | shade) : 1.0\n",
            "P(cringe-inducing | grey) : 0.5\n",
            "P(film | grey) : 0.5\n",
            "P(attempt | cringe-inducing) : 1.0\n",
            "P(romance | attempt) : 1.0\n",
            "P(poorly | romance) : 1.0\n",
            "P(written | poorly) : 0.5\n",
            "P(conceived | poorly) : 0.5\n",
            "P(dialogue | written) : 1.0\n",
            "P(unconvincing | dialogue) : 0.3333333333333333\n",
            "P(turned | dialogue) : 0.3333333333333333\n",
            "P(cringeworthy | dialogue) : 0.3333333333333333\n",
            "P(chemistry | unconvincing) : 1.0\n",
            "P(lead | chemistry) : 0.5\n",
            "P(dialogue | chemistry) : 0.5\n",
            "P(make | lead) : 1.0\n",
            "P(awkward | make) : 1.0\n",
            "P(unfulfilling | awkward) : 1.0\n",
            "P(cinematic | unfulfilling) : 1.0\n",
            "P(experience | cinematic) : 0.6666666666666666\n",
            "P(history | cinematic) : 0.3333333333333333\n",
            "P(</S> | experience) : 0.5\n",
            "P(viewer | experience) : 0.5\n",
            "P(jill | jack) : 1.0\n",
            "P(unbearable | jill) : 1.0\n",
            "P(comedy | unbearable) : 1.0\n",
            "P(relies | comedy) : 0.5\n",
            "P(pairing | comedy) : 0.5\n",
            "P(stale | relies) : 1.0\n",
            "P(humor | stale) : 1.0\n",
            "P(unfunny | painfully) : 1.0\n",
            "P(portrayal | unfunny) : 1.0\n",
            "P(adam | portrayal) : 0.5\n",
            "P(relationship | portrayal) : 0.5\n",
            "P(sandler | adam) : 1.0\n",
            "P(dual | sandler) : 1.0\n",
            "P(role | dual) : 1.0\n",
            "P(prime | role) : 1.0\n",
            "P(example | prime) : 1.0\n",
            "P(lazy | example) : 1.0\n",
            "P(filmmaking | lazy) : 1.0\n",
            "P(</S> | filmmaking) : 1.0\n",
            "P(iv | superman) : 1.0\n",
            "P(colossal | iv) : 1.0\n",
            "P(disappointment | colossal) : 1.0\n",
            "P(marred | disappointment) : 1.0\n",
            "P(low | marred) : 1.0\n",
            "P(budget | low) : 1.0\n",
            "P(laughable | budget) : 1.0\n",
            "P(special | laughable) : 1.0\n",
            "P(story | conceived) : 1.0\n",
            "P(even | story) : 1.0\n",
            "P(christopher | even) : 1.0\n",
            "P(reeve | christopher) : 1.0\n",
            "P(charm | reeve) : 1.0\n",
            "P(ca | charm) : 0.5\n",
            "P(simplicity | charm) : 0.5\n",
            "P(n't | ca) : 1.0\n",
            "P(save | n't) : 1.0\n",
            "P(mess | save) : 1.0\n",
            "P(</S> | mess) : 0.5\n",
            "P(hammy | mess) : 0.5\n",
            "P(hat | cat) : 1.0\n",
            "P(chaotic | hat) : 1.0\n",
            "P(misguided | chaotic) : 1.0\n",
            "P(adaptation | misguided) : 1.0\n",
            "P(seuss | dr) : 1.0\n",
            "P(classic | seuss) : 1.0\n",
            "P(sacrifice | classic) : 0.5\n",
            "P(right | classic) : 0.5\n",
            "P(charm | sacrifice) : 1.0\n",
            "P(source | simplicity) : 1.0\n",
            "P(material | source) : 1.0\n",
            "P(crude | material) : 1.0\n",
            "P(humor | crude) : 1.0\n",
            "P(narrative | lackluster) : 1.0\n",
            "P(</S> | narrative) : 1.0\n",
            "P(widely | room) : 1.0\n",
            "P(regarded | widely) : 0.5\n",
            "P(movie | widely) : 0.5\n",
            "P(one | regarded) : 1.0\n",
            "P(worst | one) : 1.0\n",
            "P(film | worst) : 1.0\n",
            "P(made | ever) : 1.0\n",
            "P(disjointed | made) : 1.0\n",
            "P(plot | disjointed) : 1.0\n",
            "P(acting | stilted) : 1.0\n",
            "P(dialogue | bizarre) : 1.0\n",
            "P(cult | turned) : 1.0\n",
            "P(classic | cult) : 1.0\n",
            "P(reason | right) : 1.0\n",
            "P(</S> | reason) : 1.0\n",
            "P(earth | battlefield) : 1.0\n",
            "P(sci-fi | earth) : 1.0\n",
            "P(disaster | sci-fi) : 1.0\n",
            "P(acting | hammy) : 1.0\n",
            "P(buried | remained) : 1.0\n",
            "P(annals | buried) : 1.0\n",
            "P(cinematic | annals) : 1.0\n",
            "P(</S> | history) : 1.0\n",
            "P(train | gigli) : 1.0\n",
            "P(wreck | train) : 1.0\n",
            "P(romantic | wreck) : 1.0\n",
            "P(comedy | romantic) : 1.0\n",
            "P(ben | pairing) : 1.0\n",
            "P(affleck | ben) : 1.0\n",
            "P(jennifer | affleck) : 1.0\n",
            "P(lopez | jennifer) : 1.0\n",
            "P(devoid | lopez) : 1.0\n",
            "P(chemistry | devoid) : 1.0\n",
            "P(embarrassing | cringeworthy) : 1.0\n",
            "P(misstep | embarrassing) : 1.0\n",
            "P(career | misstep) : 1.0\n",
            "P(</S> | career) : 1.0\n",
            "P(enthusiast | divided) : 1.0\n",
            "P(critic | enthusiast) : 1.0\n",
            "P(share | critic) : 1.0\n",
            "P(devoted | share) : 1.0\n",
            "P(fan | devoted) : 1.0\n",
            "P(face | also) : 1.0\n",
            "P(substantial | face) : 1.0\n",
            "P(criticism | substantial) : 1.0\n",
            "P(film | criticism) : 1.0\n",
            "P(execution | content) : 1.0\n",
            "P(leave | execution) : 1.0\n",
            "P(much | leave) : 0.5\n",
            "P(many | leave) : 0.5\n",
            "P(desired | much) : 1.0\n",
            "P(making | desired) : 1.0\n",
            "P(cinematic | polarizing) : 1.0\n",
            "P(reaction | viewer) : 0.5\n",
            "P(uncomfortable | viewer) : 0.5\n",
            "P(may | reaction) : 1.0\n",
            "P(vary | may) : 0.3333333333333333\n",
            "P(leave | may) : 0.3333333333333333\n",
            "P(find | may) : 0.3333333333333333\n",
            "P(widely | vary) : 1.0\n",
            "P(strong | evokes) : 1.0\n",
            "P(opinion | strong) : 1.0\n",
            "P(end | opinion) : 1.0\n",
            "P(spectrum | end) : 1.0\n",
            "P(</S> | spectrum) : 1.0\n",
            "P(theme | relationship) : 1.0\n",
            "P(may | theme) : 1.0\n",
            "P(viewer | many) : 1.0\n",
            "P(test | uncomfortable) : 1.0\n",
            "P(limit | test) : 1.0\n",
            "P(may | limit) : 1.0\n",
            "P(acceptable | find) : 1.0\n",
            "P(mainstream | acceptable) : 1.0\n",
            "P(cinema | mainstream) : 1.0\n",
            "P(idea | cinema) : 1.0\n",
            "P(keeping | idea) : 1.0\n",
            "P(open | keeping) : 1.0\n",
            "P(mind | open) : 1.0\n",
            "P(essential | mind) : 1.0\n",
            "P(film-watching | essential) : 1.0\n",
            "P(particular | film-watching) : 1.0\n",
            "P(movie | particular) : 1.0\n",
            "P(push | might) : 1.0\n",
            "P(openness | push) : 1.0\n",
            "P(boundary | openness) : 1.0\n",
            "P(</S> | boundary) : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 5: Bi-gram probability for the following test movie review"
      ],
      "metadata": {
        "id": "grUGrGzdTQzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to deal with out of vocabulary (OOV) words, I have applied Add-K smoothing. To find better value for k, we need validation dataset."
      ],
      "metadata": {
        "id": "TbVoVGCXmHjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate the probability of a review using bigram probabilities with Laplace smoothing\n",
        "def calculate_review_probability(review, bigram_count, vocabulary_size, k=0.1):\n",
        "    probability = 1.0  # Initialize the probability\n",
        "    #k=0.1 laplaccian smoothing - Add one smoothing\n",
        "\n",
        "    review = ['<S>'] + review + ['</S>']\n",
        "\n",
        "    # Calculate the probability by multiplying individual bigram probabilities with Laplace smoothing\n",
        "    for i in range(len(review) - 1):  # Use the length of the review\n",
        "        current_word = review[i]\n",
        "        next_word = review[i + 1]\n",
        "        bigram = (current_word, next_word)\n",
        "\n",
        "        # Print to check values\n",
        "        print(f'Count({next_word} | {current_word}) : {bigram_count[current_word][next_word]}')\n",
        "\n",
        "        # Apply Laplace smoothing by adding k to both the numerator and denominator\n",
        "        current_word_count = sum(bigram_count[current_word].values())  # Sum of counts for the current word\n",
        "        print(f'Count of {current_word}: {current_word_count}')\n",
        "        probability *= (bigram_count[current_word][next_word] + k) / (k * vocabulary_size + current_word_count)\n",
        "\n",
        "        # Print to check values\n",
        "        print(f'Laplace P({next_word} | {current_word}) : {(bigram_count[current_word][next_word] + k) / (k * vocabulary_size + current_word_count)}')\n",
        "\n",
        "        print()\n",
        "    return probability"
      ],
      "metadata": {
        "id": "eg9qG2shUEeW"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probability for the test review with respect to positive and negative datasets\n",
        "positive_review_probability_bigram = calculate_review_probability(preprocessed_test_review, positive_bigram_count, vocabulary_size_positive)"
      ],
      "metadata": {
        "id": "fAx0HIorV9W2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7231c164-da3f-4410-d70c-ce80d6d8ff78"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count(clear | <S>) : 0\n",
            "Count of <S>: 12\n",
            "Laplace P(clear | <S>) : 0.0028653295128939823\n",
            "\n",
            "Count(movie | clear) : 0\n",
            "Count of clear: 1\n",
            "Laplace P(movie | clear) : 0.0041841004184100415\n",
            "\n",
            "Count(enthusiast | movie) : 0\n",
            "Count of movie: 1\n",
            "Laplace P(enthusiast | movie) : 0.0041841004184100415\n",
            "\n",
            "Count(critic | enthusiast) : 1\n",
            "Count of enthusiast: 1\n",
            "Laplace P(critic | enthusiast) : 0.04602510460251046\n",
            "\n",
            "Count(may | critic) : 0\n",
            "Count of critic: 1\n",
            "Laplace P(may | critic) : 0.0041841004184100415\n",
            "\n",
            "Count(everyone | may) : 0\n",
            "Count of may: 1\n",
            "Laplace P(everyone | may) : 0.0041841004184100415\n",
            "\n",
            "Count(taste | everyone) : 0\n",
            "Count of everyone: 1\n",
            "Laplace P(taste | everyone) : 0.0041841004184100415\n",
            "\n",
            "Count(worth | taste) : 0\n",
            "Count of taste: 0\n",
            "Laplace P(worth | taste) : 0.004366812227074236\n",
            "\n",
            "Count(watching | worth) : 0\n",
            "Count of worth: 0\n",
            "Laplace P(watching | worth) : 0.004366812227074236\n",
            "\n",
            "Count(open | watching) : 0\n",
            "Count of watching: 0\n",
            "Laplace P(open | watching) : 0.004366812227074236\n",
            "\n",
            "Count(mind | open) : 1\n",
            "Count of open: 1\n",
            "Laplace P(mind | open) : 0.04602510460251046\n",
            "\n",
            "Count(form | mind) : 0\n",
            "Count of mind: 2\n",
            "Laplace P(form | mind) : 0.004016064257028112\n",
            "\n",
            "Count(opinion | form) : 0\n",
            "Count of form: 0\n",
            "Laplace P(opinion | form) : 0.004366812227074236\n",
            "\n",
            "Count(</S> | opinion) : 0\n",
            "Count of opinion: 1\n",
            "Laplace P(</S> | opinion) : 0.0041841004184100415\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_review_probability_bigram = calculate_review_probability(preprocessed_test_review, negative_bigram_count, vocabulary_size_negative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew81o5ArEtWL",
        "outputId": "54ef3b69-fe08-4206-ddd2-b35864c00248"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count(clear | <S>) : 0\n",
            "Count of <S>: 12\n",
            "Laplace P(clear | <S>) : 0.0031645569620253164\n",
            "\n",
            "Count(movie | clear) : 0\n",
            "Count of clear: 0\n",
            "Laplace P(movie | clear) : 0.00510204081632653\n",
            "\n",
            "Count(enthusiast | movie) : 0\n",
            "Count of movie: 4\n",
            "Laplace P(enthusiast | movie) : 0.00423728813559322\n",
            "\n",
            "Count(critic | enthusiast) : 1\n",
            "Count of enthusiast: 1\n",
            "Laplace P(critic | enthusiast) : 0.05339805825242719\n",
            "\n",
            "Count(may | critic) : 0\n",
            "Count of critic: 1\n",
            "Laplace P(may | critic) : 0.0048543689320388345\n",
            "\n",
            "Count(everyone | may) : 0\n",
            "Count of may: 3\n",
            "Laplace P(everyone | may) : 0.004424778761061947\n",
            "\n",
            "Count(taste | everyone) : 0\n",
            "Count of everyone: 0\n",
            "Laplace P(taste | everyone) : 0.00510204081632653\n",
            "\n",
            "Count(worth | taste) : 0\n",
            "Count of taste: 0\n",
            "Laplace P(worth | taste) : 0.00510204081632653\n",
            "\n",
            "Count(watching | worth) : 0\n",
            "Count of worth: 0\n",
            "Laplace P(watching | worth) : 0.00510204081632653\n",
            "\n",
            "Count(open | watching) : 0\n",
            "Count of watching: 0\n",
            "Laplace P(open | watching) : 0.00510204081632653\n",
            "\n",
            "Count(mind | open) : 1\n",
            "Count of open: 1\n",
            "Laplace P(mind | open) : 0.05339805825242719\n",
            "\n",
            "Count(form | mind) : 0\n",
            "Count of mind: 1\n",
            "Laplace P(form | mind) : 0.0048543689320388345\n",
            "\n",
            "Count(opinion | form) : 0\n",
            "Count of form: 0\n",
            "Laplace P(opinion | form) : 0.00510204081632653\n",
            "\n",
            "Count(</S> | opinion) : 0\n",
            "Count of opinion: 1\n",
            "Laplace P(</S> | opinion) : 0.0048543689320388345\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 6: Predict the category of the test movie review using Bi-Gram model"
      ],
      "metadata": {
        "id": "3yF1DKi4bWpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the probabilities\n",
        "print(\"Positive Review Probability:\", positive_review_probability_bigram)\n",
        "print(\"Negative Review Probability:\", negative_review_probability_bigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8MnCW26XDau",
        "outputId": "09b763f5-9ee8-446b-a662-bf3f1dc0d114"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Review Probability: 4.7559300599267804e-32\n",
            "Negative Review Probability: 3.413542509672997e-31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the probabilities and make a classification decision based on the task\n",
        "if positive_review_probability_bigram > negative_review_probability_bigram:\n",
        "    print(\"Predicted: Positive\")\n",
        "else:\n",
        "    print(\"Predicted: Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBjCtJVgXI0u",
        "outputId": "35aeea05-d527-431d-95b3-ac8a4b618077"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is the Negative review, since it has the high probability of being assigned in negative class according to bigram model."
      ],
      "metadata": {
        "id": "cqN4jIwRESeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 7:  Perplexity"
      ],
      "metadata": {
        "id": "Ai8EPWbGcAWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have explained the concept of perplexity in the pdf document which I have submitted for Part A answer"
      ],
      "metadata": {
        "id": "Tv_kux5mRQ5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert positive reviews to a set of tuples (converted sentences)\n",
        "positive_reviews_set = {tuple(review) for review in preprocessed_positive_reviews}\n",
        "\n",
        "# Convert the preprocessed test review to a tuple\n",
        "test_review_tuple = tuple(preprocessed_test_review)\n",
        "\n",
        "# Check for the overlap between train data and test data - Since having same sentence will give low value for perplexity\n",
        "if test_review_tuple in positive_reviews_set:\n",
        "    print(\"Test review overlaps with positive reviews in the training dataset.\")\n",
        "else:\n",
        "    print(\"No overlap found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY2aQSnQWBPE",
        "outputId": "71a8b77d-e3a9-476e-84bf-32febb20dab4"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No overlap found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert negative reviews to a set of tuples (converted sentences)\n",
        "negative_reviews_set = {tuple(review) for review in preprocessed_negative_reviews}\n",
        "\n",
        "# Convert the preprocessed test review to a tuple\n",
        "test_review_tuple = tuple(preprocessed_test_review)\n",
        "\n",
        "# Check for the overlap between train data and test data - Since having same sentence will give low value for perplexity\n",
        "if test_review_tuple in negative_reviews_set:\n",
        "    print(\"Test review overlaps with positive reviews in the training dataset.\")\n",
        "else:\n",
        "    print(\"No overlap found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVU7EtPJWQMG",
        "outputId": "3a81f4cc-ab60-4bc7-c736-a957c199ce60"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No overlap found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(review, bigram_count, vocabulary_size, k=0.1):\n",
        "    probability = 1.0\n",
        "    word_count = 0\n",
        "\n",
        "    review = ['<S>'] + review + ['</S>']\n",
        "\n",
        "    for i in range(len(review) - 1):\n",
        "        current_word = review[i]\n",
        "        next_word = review[i + 1]\n",
        "\n",
        "        # Apply Laplace smoothing by adding k to both the numerator and denominator\n",
        "        current_word_count = sum(bigram_count[current_word].values())  # Sum of counts for the current word\n",
        "\n",
        "        # Compute the conditional probability using Laplace smoothing\n",
        "        conditional_probability = (bigram_count[current_word][next_word] + k) / (k * vocabulary_size + current_word_count)\n",
        "\n",
        "        # Update perplexity\n",
        "        probability *= conditional_probability\n",
        "\n",
        "        # Exclude <S> from the word count\n",
        "        if current_word != '<S>':\n",
        "            word_count += 1\n",
        "\n",
        "    # Compute perplexity as the inverse probability normalized by the word count\n",
        "    perplexity = math.pow(1 / probability, 1 / word_count)\n",
        "\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "AUxTXTbJb9yP"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for the positive and negative test reviews\n",
        "perplexity_positive = calculate_perplexity(preprocessed_test_review, positive_bigram_count, vocabulary_size_positive)\n",
        "perplexity_negative = calculate_perplexity(preprocessed_test_review, negative_bigram_count, vocabulary_size_negative)\n",
        "\n",
        "print(f\"Perplexity for positive test review: {perplexity_positive}\")\n",
        "print(f\"Perplexity for negative test review: {perplexity_negative}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT6nmR5Nqx12",
        "outputId": "7c9053d5-6779-4d04-add7-34e7d336c6cc"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for positive test review: 256.7103780050521\n",
            "Perplexity for negative test review: 220.5969944806656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimizing Perplexity is same as Maximizing probability of the sentence."
      ],
      "metadata": {
        "id": "3jqna9KE7f6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prdeicting test review using Backoff smoothing"
      ],
      "metadata": {
        "id": "QFjjwEwM04i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_review_probability(review, bigram_count, vocabulary_size):\n",
        "    probability = 1.0  # Initialize the probability\n",
        "\n",
        "    review = ['<S>'] + review + ['</S>']\n",
        "\n",
        "    # Calculate the probability by multiplying individual bigram probabilities\n",
        "    for i in range(len(review) - 1):\n",
        "        current_word = review[i]\n",
        "        next_word = review[i + 1]\n",
        "        bigram = (current_word, next_word)\n",
        "\n",
        "        # Check if the bigram exists in the bigram count dictionary\n",
        "        if bigram in bigram_count[current_word]:\n",
        "            probability *= bigram_count[current_word][bigram] / sum(bigram_count[current_word].values())\n",
        "        else:\n",
        "            # Backoff to unigram probability\n",
        "            probability *= 1 / vocabulary_size\n",
        "\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "xBLvCY_i0H4S"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probability for the test review with respect to positive and negative datasets\n",
        "positive_review_probability_bigram = calculate_review_probability(preprocessed_test_review, positive_bigram_count, vocabulary_size_positive)\n",
        "negative_review_probability_bigram = calculate_review_probability(preprocessed_test_review, negative_bigram_count, vocabulary_size_negative)"
      ],
      "metadata": {
        "id": "RqP6mYFV0oWv"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the probabilities\n",
        "print(\"Positive Review Probability:\", positive_review_probability_bigram)\n",
        "print(\"Negative Review Probability:\", negative_review_probability_bigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpEH63kB0wL3",
        "outputId": "fff41165-a05f-4a19-eab9-3adffb2cbf65"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Review Probability: 9.168604668478727e-34\n",
            "Negative Review Probability: 8.098694206237001e-33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the probabilities and make a classification decision based on the task\n",
        "if positive_review_probability_bigram > negative_review_probability_bigram:\n",
        "    print(\"Predicted: Positive\")\n",
        "else:\n",
        "    print(\"Predicted: Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjWuaBkP02j7",
        "outputId": "e0aa9716-486e-445f-d7a0-7e201f23d0d5"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(review, bigram_count, vocabulary_size, k=0.1):\n",
        "    probability = 1.0\n",
        "    word_count = 0\n",
        "\n",
        "    review = ['<S>'] + review + ['</S>']\n",
        "\n",
        "    for i in range(len(review) - 1):\n",
        "        current_word = review[i]\n",
        "        next_word = review[i + 1]\n",
        "        bigram = (current_word, next_word)\n",
        "\n",
        "        # Check if the bigram exists in the bigram count dictionary\n",
        "        if bigram in bigram_count[current_word]:\n",
        "            probability *= bigram_count[current_word][bigram] / sum(bigram_count[current_word].values())\n",
        "        else:\n",
        "            # Backoff to unigram probability\n",
        "            probability *= 1 / vocabulary_size\n",
        "\n",
        "\n",
        "        # Exclude <S> from the word count\n",
        "        if current_word != '<S>':\n",
        "            word_count += 1\n",
        "\n",
        "    # Compute perplexity as the inverse probability normalized by the word count\n",
        "    perplexity = math.pow(1 / probability, 1 / word_count)\n",
        "\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "-tFcxH-A_RCx"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for the positive and negative test reviews\n",
        "perplexity_positive = calculate_perplexity(preprocessed_test_review, positive_bigram_count, vocabulary_size_positive)\n",
        "perplexity_negative = calculate_perplexity(preprocessed_test_review, negative_bigram_count, vocabulary_size_negative)\n",
        "\n",
        "print(f\"Perplexity for positive test review: {perplexity_positive}\")\n",
        "print(f\"Perplexity for negative test review: {perplexity_negative}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaaWD0x__qr_",
        "outputId": "4ee73377-83f1-41d2-ea92-f334cda96a77"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for positive test review: 347.8253963413587\n",
            "Perplexity for negative test review: 294.15989474319053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using Backoff smoothing, it has the high perplexity value compare to the method using Add-K smoothing.\n",
        "\n",
        "Having low perplexity is good to maximize the probability. So this smoothing technique is not good."
      ],
      "metadata": {
        "id": "KNjZV_tjBQZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unigram"
      ],
      "metadata": {
        "id": "u-XkvqGMRIR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 3: Implementation of Uni-Gram model"
      ],
      "metadata": {
        "id": "C59ZyN22YYzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unigram_model(reviews):\n",
        "    unigram_counts = defaultdict(int)\n",
        "    total_words = 0\n",
        "\n",
        "    for review in reviews:\n",
        "\n",
        "        for word in review:\n",
        "            unigram_counts[word] += 1\n",
        "            total_words += 1\n",
        "\n",
        "    # Calculate unigram probabilities\n",
        "    unigram_probabilities = defaultdict(float)\n",
        "    unigram_count = defaultdict(int)\n",
        "    for word, count in unigram_counts.items():\n",
        "        unigram_count[word] = count\n",
        "        unigram_probabilities[word] = count / total_words\n",
        "\n",
        "    return unigram_probabilities, unigram_count"
      ],
      "metadata": {
        "id": "OJ2oO8SxRL2j"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unigram models for the positive and negative train sets\n",
        "positive_train_unigram_model, positive_unigram_count = build_unigram_model(preprocessed_positive_reviews)\n",
        "negative_train_unigram_model, negative_unigram_count = build_unigram_model(preprocessed_negative_reviews)"
      ],
      "metadata": {
        "id": "xlUxbOLtRnXj"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unigram Counts:\")\n",
        "for word, count in positive_unigram_count.items():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Wv5STrmwsP",
        "outputId": "809555b2-5132-4eda-e944-d61c94ccc4d8"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Counts:\n",
            "forrest: 1\n",
            "gump: 1\n",
            "absolute: 1\n",
            "masterpiece: 3\n",
            "tom: 1\n",
            "hank: 1\n",
            "delivers: 1\n",
            "unforgettable: 1\n",
            "performance: 2\n",
            "storytelling: 1\n",
            "heartwarming: 1\n",
            "movie: 1\n",
            "journey: 2\n",
            "life: 1\n",
            "make: 5\n",
            "laugh: 1\n",
            "cry: 1\n",
            "appreciate: 1\n",
            "simple: 1\n",
            "beauty: 1\n",
            "existence: 1\n",
            "shawshank: 1\n",
            "redemption: 2\n",
            "timeless: 2\n",
            "classic: 2\n",
            "powerful: 1\n",
            "theme: 1\n",
            "hope: 1\n",
            "friendship: 1\n",
            "must-watch: 1\n",
            "morgan: 1\n",
            "freeman: 1\n",
            "tim: 1\n",
            "robbins: 1\n",
            "give: 1\n",
            "exceptional: 1\n",
            "brilliantly: 1\n",
            "crafted: 1\n",
            "film: 8\n",
            "epic: 2\n",
            "conclusion: 1\n",
            "lord: 1\n",
            "ring: 1\n",
            "trilogy: 1\n",
            "return: 1\n",
            "king: 1\n",
            "cinematic: 2\n",
            "triumph: 1\n",
            "breathtaking: 1\n",
            "visuals: 1\n",
            "battle: 1\n",
            "emotionally: 1\n",
            "resonant: 1\n",
            "story: 2\n",
            "monumental: 1\n",
            "achievement: 1\n",
            "filmmaking: 1\n",
            "la: 2\n",
            "land: 1\n",
            "love: 3\n",
            "letter: 1\n",
            "magic: 1\n",
            "hollywood: 1\n",
            "dream: 2\n",
            "chemistry: 2\n",
            "ryan: 1\n",
            "gosling: 1\n",
            "emma: 1\n",
            "stone: 1\n",
            "enchanting: 1\n",
            "music: 1\n",
            "dance: 1\n",
            "sequence: 1\n",
            "pure: 1\n",
            "delight: 2\n",
            "modern: 2\n",
            "musical: 1\n",
            "wes: 1\n",
            "anderson: 1\n",
            "whimsical: 1\n",
            "style: 1\n",
            "shine: 2\n",
            "grand: 1\n",
            "budapest: 1\n",
            "hotel: 1\n",
            "quirky: 1\n",
            "character: 2\n",
            "colorful: 1\n",
            "cinematography: 1\n",
            "visual: 2\n",
            "narrative: 2\n",
            "charming: 1\n",
            "delightful: 2\n",
            "experience: 2\n",
            "inception: 1\n",
            "mind-bending: 1\n",
            "brilliance: 1\n",
            "christopher: 1\n",
            "nolan: 1\n",
            "intricate: 1\n",
            "plot: 1\n",
            "stunning: 1\n",
            "effect: 1\n",
            "han: 1\n",
            "zimmer: 1\n",
            "haunting: 1\n",
            "score: 1\n",
            "create: 1\n",
            "keep: 1\n",
            "edge: 1\n",
            "seat: 1\n",
            "true: 1\n",
            "sci-fi: 1\n",
            "cinema: 2\n",
            "social: 1\n",
            "network: 1\n",
            "captivating: 1\n",
            "exploration: 2\n",
            "creation: 1\n",
            "facebook: 1\n",
            "personal: 1\n",
            "legal: 1\n",
            "conflict: 1\n",
            "ensued: 1\n",
            "david: 1\n",
            "fincher: 1\n",
            "direction: 1\n",
            "aaron: 1\n",
            "sorkin: 1\n",
            "sharp: 1\n",
            "screenplay: 1\n",
            "smith: 1\n",
            "portrayal: 1\n",
            "chris: 1\n",
            "gardner: 1\n",
            "pursuit: 1\n",
            "happyness: 1\n",
            "touching: 1\n",
            "inspirational: 1\n",
            "reminds: 1\n",
            "u: 1\n",
            "determination: 1\n",
            "unwavering: 1\n",
            "spirit: 1\n",
            "anyone: 1\n",
            "overcome: 1\n",
            "adversity: 1\n",
            "achieve: 1\n",
            "eternal: 1\n",
            "sunshine: 1\n",
            "spotless: 1\n",
            "mind: 2\n",
            "beautifully: 1\n",
            "unconventional: 1\n",
            "jim: 1\n",
            "carrey: 1\n",
            "kate: 1\n",
            "winslet: 1\n",
            "role: 1\n",
            "told: 1\n",
            "non-linear: 1\n",
            "fashion: 1\n",
            "poignant: 1\n",
            "memory: 1\n",
            "human: 1\n",
            "connection: 1\n",
            "princess: 1\n",
            "bride: 1\n",
            "fairy: 1\n",
            "tale: 1\n",
            "perfect: 1\n",
            "blend: 1\n",
            "humor: 1\n",
            "romance: 2\n",
            "adventure: 1\n",
            "witty: 1\n",
            "dialogue: 1\n",
            "memorable: 1\n",
            "appeal: 1\n",
            "kid: 1\n",
            "adult: 1\n",
            "inconceivably: 1\n",
            "fifty: 1\n",
            "shade: 1\n",
            "grey: 1\n",
            "managed: 2\n",
            "captivate: 1\n",
            "enthusiast: 1\n",
            "critic: 1\n",
            "alike: 1\n",
            "ability: 1\n",
            "spark: 1\n",
            "passionate: 1\n",
            "discussion: 1\n",
            "elicit: 1\n",
            "wide: 1\n",
            "range: 1\n",
            "opinion: 1\n",
            "testament: 1\n",
            "impact: 1\n",
            "may: 1\n",
            "find: 1\n",
            "controversial: 1\n",
            "denying: 1\n",
            "left: 1\n",
            "significant: 1\n",
            "mark: 1\n",
            "world: 2\n",
            "conversation: 1\n",
            "starter: 1\n",
            "clear: 1\n",
            "wo: 1\n",
            "n't: 1\n",
            "everyone: 1\n",
            "cup: 1\n",
            "tea: 1\n",
            "willing: 1\n",
            "approach: 1\n",
            "open: 1\n",
            "visually: 1\n",
            "lush: 1\n",
            "moment: 1\n",
            "genuine: 1\n",
            "lead: 1\n",
            "offer: 1\n",
            "something: 1\n",
            "different: 1\n",
            "intriguing: 1\n",
            "drama: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 4: Uni-Gram probability"
      ],
      "metadata": {
        "id": "EPqHZIHLSMup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unigram probabilities for each word\n",
        "for word, probability in positive_train_unigram_model.items():\n",
        "    print(f\"{word}: {probability}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DK7jPoNSSnZ",
        "outputId": "cb6bdc91-4a81-4f90-e12b-10bc3fa95f67"
      },
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forrest: 0.0037174721189591076\n",
            "gump: 0.0037174721189591076\n",
            "absolute: 0.0037174721189591076\n",
            "masterpiece: 0.011152416356877323\n",
            "tom: 0.0037174721189591076\n",
            "hank: 0.0037174721189591076\n",
            "delivers: 0.0037174721189591076\n",
            "unforgettable: 0.0037174721189591076\n",
            "performance: 0.007434944237918215\n",
            "storytelling: 0.0037174721189591076\n",
            "heartwarming: 0.0037174721189591076\n",
            "movie: 0.0037174721189591076\n",
            "journey: 0.007434944237918215\n",
            "life: 0.0037174721189591076\n",
            "make: 0.01858736059479554\n",
            "laugh: 0.0037174721189591076\n",
            "cry: 0.0037174721189591076\n",
            "appreciate: 0.0037174721189591076\n",
            "simple: 0.0037174721189591076\n",
            "beauty: 0.0037174721189591076\n",
            "existence: 0.0037174721189591076\n",
            "shawshank: 0.0037174721189591076\n",
            "redemption: 0.007434944237918215\n",
            "timeless: 0.007434944237918215\n",
            "classic: 0.007434944237918215\n",
            "powerful: 0.0037174721189591076\n",
            "theme: 0.0037174721189591076\n",
            "hope: 0.0037174721189591076\n",
            "friendship: 0.0037174721189591076\n",
            "must-watch: 0.0037174721189591076\n",
            "morgan: 0.0037174721189591076\n",
            "freeman: 0.0037174721189591076\n",
            "tim: 0.0037174721189591076\n",
            "robbins: 0.0037174721189591076\n",
            "give: 0.0037174721189591076\n",
            "exceptional: 0.0037174721189591076\n",
            "brilliantly: 0.0037174721189591076\n",
            "crafted: 0.0037174721189591076\n",
            "film: 0.02973977695167286\n",
            "epic: 0.007434944237918215\n",
            "conclusion: 0.0037174721189591076\n",
            "lord: 0.0037174721189591076\n",
            "ring: 0.0037174721189591076\n",
            "trilogy: 0.0037174721189591076\n",
            "return: 0.0037174721189591076\n",
            "king: 0.0037174721189591076\n",
            "cinematic: 0.007434944237918215\n",
            "triumph: 0.0037174721189591076\n",
            "breathtaking: 0.0037174721189591076\n",
            "visuals: 0.0037174721189591076\n",
            "battle: 0.0037174721189591076\n",
            "emotionally: 0.0037174721189591076\n",
            "resonant: 0.0037174721189591076\n",
            "story: 0.007434944237918215\n",
            "monumental: 0.0037174721189591076\n",
            "achievement: 0.0037174721189591076\n",
            "filmmaking: 0.0037174721189591076\n",
            "la: 0.007434944237918215\n",
            "land: 0.0037174721189591076\n",
            "love: 0.011152416356877323\n",
            "letter: 0.0037174721189591076\n",
            "magic: 0.0037174721189591076\n",
            "hollywood: 0.0037174721189591076\n",
            "dream: 0.007434944237918215\n",
            "chemistry: 0.007434944237918215\n",
            "ryan: 0.0037174721189591076\n",
            "gosling: 0.0037174721189591076\n",
            "emma: 0.0037174721189591076\n",
            "stone: 0.0037174721189591076\n",
            "enchanting: 0.0037174721189591076\n",
            "music: 0.0037174721189591076\n",
            "dance: 0.0037174721189591076\n",
            "sequence: 0.0037174721189591076\n",
            "pure: 0.0037174721189591076\n",
            "delight: 0.007434944237918215\n",
            "modern: 0.007434944237918215\n",
            "musical: 0.0037174721189591076\n",
            "wes: 0.0037174721189591076\n",
            "anderson: 0.0037174721189591076\n",
            "whimsical: 0.0037174721189591076\n",
            "style: 0.0037174721189591076\n",
            "shine: 0.007434944237918215\n",
            "grand: 0.0037174721189591076\n",
            "budapest: 0.0037174721189591076\n",
            "hotel: 0.0037174721189591076\n",
            "quirky: 0.0037174721189591076\n",
            "character: 0.007434944237918215\n",
            "colorful: 0.0037174721189591076\n",
            "cinematography: 0.0037174721189591076\n",
            "visual: 0.007434944237918215\n",
            "narrative: 0.007434944237918215\n",
            "charming: 0.0037174721189591076\n",
            "delightful: 0.007434944237918215\n",
            "experience: 0.007434944237918215\n",
            "inception: 0.0037174721189591076\n",
            "mind-bending: 0.0037174721189591076\n",
            "brilliance: 0.0037174721189591076\n",
            "christopher: 0.0037174721189591076\n",
            "nolan: 0.0037174721189591076\n",
            "intricate: 0.0037174721189591076\n",
            "plot: 0.0037174721189591076\n",
            "stunning: 0.0037174721189591076\n",
            "effect: 0.0037174721189591076\n",
            "han: 0.0037174721189591076\n",
            "zimmer: 0.0037174721189591076\n",
            "haunting: 0.0037174721189591076\n",
            "score: 0.0037174721189591076\n",
            "create: 0.0037174721189591076\n",
            "keep: 0.0037174721189591076\n",
            "edge: 0.0037174721189591076\n",
            "seat: 0.0037174721189591076\n",
            "true: 0.0037174721189591076\n",
            "sci-fi: 0.0037174721189591076\n",
            "cinema: 0.007434944237918215\n",
            "social: 0.0037174721189591076\n",
            "network: 0.0037174721189591076\n",
            "captivating: 0.0037174721189591076\n",
            "exploration: 0.007434944237918215\n",
            "creation: 0.0037174721189591076\n",
            "facebook: 0.0037174721189591076\n",
            "personal: 0.0037174721189591076\n",
            "legal: 0.0037174721189591076\n",
            "conflict: 0.0037174721189591076\n",
            "ensued: 0.0037174721189591076\n",
            "david: 0.0037174721189591076\n",
            "fincher: 0.0037174721189591076\n",
            "direction: 0.0037174721189591076\n",
            "aaron: 0.0037174721189591076\n",
            "sorkin: 0.0037174721189591076\n",
            "sharp: 0.0037174721189591076\n",
            "screenplay: 0.0037174721189591076\n",
            "smith: 0.0037174721189591076\n",
            "portrayal: 0.0037174721189591076\n",
            "chris: 0.0037174721189591076\n",
            "gardner: 0.0037174721189591076\n",
            "pursuit: 0.0037174721189591076\n",
            "happyness: 0.0037174721189591076\n",
            "touching: 0.0037174721189591076\n",
            "inspirational: 0.0037174721189591076\n",
            "reminds: 0.0037174721189591076\n",
            "u: 0.0037174721189591076\n",
            "determination: 0.0037174721189591076\n",
            "unwavering: 0.0037174721189591076\n",
            "spirit: 0.0037174721189591076\n",
            "anyone: 0.0037174721189591076\n",
            "overcome: 0.0037174721189591076\n",
            "adversity: 0.0037174721189591076\n",
            "achieve: 0.0037174721189591076\n",
            "eternal: 0.0037174721189591076\n",
            "sunshine: 0.0037174721189591076\n",
            "spotless: 0.0037174721189591076\n",
            "mind: 0.007434944237918215\n",
            "beautifully: 0.0037174721189591076\n",
            "unconventional: 0.0037174721189591076\n",
            "jim: 0.0037174721189591076\n",
            "carrey: 0.0037174721189591076\n",
            "kate: 0.0037174721189591076\n",
            "winslet: 0.0037174721189591076\n",
            "role: 0.0037174721189591076\n",
            "told: 0.0037174721189591076\n",
            "non-linear: 0.0037174721189591076\n",
            "fashion: 0.0037174721189591076\n",
            "poignant: 0.0037174721189591076\n",
            "memory: 0.0037174721189591076\n",
            "human: 0.0037174721189591076\n",
            "connection: 0.0037174721189591076\n",
            "princess: 0.0037174721189591076\n",
            "bride: 0.0037174721189591076\n",
            "fairy: 0.0037174721189591076\n",
            "tale: 0.0037174721189591076\n",
            "perfect: 0.0037174721189591076\n",
            "blend: 0.0037174721189591076\n",
            "humor: 0.0037174721189591076\n",
            "romance: 0.007434944237918215\n",
            "adventure: 0.0037174721189591076\n",
            "witty: 0.0037174721189591076\n",
            "dialogue: 0.0037174721189591076\n",
            "memorable: 0.0037174721189591076\n",
            "appeal: 0.0037174721189591076\n",
            "kid: 0.0037174721189591076\n",
            "adult: 0.0037174721189591076\n",
            "inconceivably: 0.0037174721189591076\n",
            "fifty: 0.0037174721189591076\n",
            "shade: 0.0037174721189591076\n",
            "grey: 0.0037174721189591076\n",
            "managed: 0.007434944237918215\n",
            "captivate: 0.0037174721189591076\n",
            "enthusiast: 0.0037174721189591076\n",
            "critic: 0.0037174721189591076\n",
            "alike: 0.0037174721189591076\n",
            "ability: 0.0037174721189591076\n",
            "spark: 0.0037174721189591076\n",
            "passionate: 0.0037174721189591076\n",
            "discussion: 0.0037174721189591076\n",
            "elicit: 0.0037174721189591076\n",
            "wide: 0.0037174721189591076\n",
            "range: 0.0037174721189591076\n",
            "opinion: 0.0037174721189591076\n",
            "testament: 0.0037174721189591076\n",
            "impact: 0.0037174721189591076\n",
            "may: 0.0037174721189591076\n",
            "find: 0.0037174721189591076\n",
            "controversial: 0.0037174721189591076\n",
            "denying: 0.0037174721189591076\n",
            "left: 0.0037174721189591076\n",
            "significant: 0.0037174721189591076\n",
            "mark: 0.0037174721189591076\n",
            "world: 0.007434944237918215\n",
            "conversation: 0.0037174721189591076\n",
            "starter: 0.0037174721189591076\n",
            "clear: 0.0037174721189591076\n",
            "wo: 0.0037174721189591076\n",
            "n't: 0.0037174721189591076\n",
            "everyone: 0.0037174721189591076\n",
            "cup: 0.0037174721189591076\n",
            "tea: 0.0037174721189591076\n",
            "willing: 0.0037174721189591076\n",
            "approach: 0.0037174721189591076\n",
            "open: 0.0037174721189591076\n",
            "visually: 0.0037174721189591076\n",
            "lush: 0.0037174721189591076\n",
            "moment: 0.0037174721189591076\n",
            "genuine: 0.0037174721189591076\n",
            "lead: 0.0037174721189591076\n",
            "offer: 0.0037174721189591076\n",
            "something: 0.0037174721189591076\n",
            "different: 0.0037174721189591076\n",
            "intriguing: 0.0037174721189591076\n",
            "drama: 0.0037174721189591076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unigram probabilities for each word\n",
        "for word, probability in negative_train_unigram_model.items():\n",
        "    print(f\"{word}: {probability}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yujfBpSTStNc",
        "outputId": "4aad5c88-a30b-4866-e7f7-5f451796e1da"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last: 0.004149377593360996\n",
            "airbender: 0.004149377593360996\n",
            "disaster: 0.008298755186721992\n",
            "film: 0.029045643153526972\n",
            "adaptation: 0.008298755186721992\n",
            "butcher: 0.004149377593360996\n",
            "beloved: 0.004149377593360996\n",
            "animated: 0.008298755186721992\n",
            "series: 0.004149377593360996\n",
            "wooden: 0.004149377593360996\n",
            "acting: 0.012448132780082987\n",
            "convoluted: 0.008298755186721992\n",
            "storytelling: 0.004149377593360996\n",
            "cringe-worthy: 0.004149377593360996\n",
            "special: 0.012448132780082987\n",
            "effect: 0.012448132780082987\n",
            "letdown: 0.004149377593360996\n",
            "fan: 0.008298755186721992\n",
            "newcomer: 0.004149377593360996\n",
            "alike: 0.004149377593360996\n",
            "another: 0.004149377593360996\n",
            "transformer: 0.004149377593360996\n",
            "movie: 0.016597510373443983\n",
            "mindless: 0.004149377593360996\n",
            "explosion: 0.004149377593360996\n",
            "incoherent: 0.004149377593360996\n",
            "plotlines: 0.004149377593360996\n",
            "overreliance: 0.004149377593360996\n",
            "cgi: 0.004149377593360996\n",
            "franchise: 0.004149377593360996\n",
            "desperately: 0.004149377593360996\n",
            "need: 0.004149377593360996\n",
            "overhaul: 0.004149377593360996\n",
            "emoji: 0.004149377593360996\n",
            "blatant: 0.004149377593360996\n",
            "cash: 0.004149377593360996\n",
            "grab: 0.004149377593360996\n",
            "shallow: 0.004149377593360996\n",
            "uninspired: 0.004149377593360996\n",
            "plot: 0.008298755186721992\n",
            "fails: 0.004149377593360996\n",
            "deliver: 0.004149377593360996\n",
            "clever: 0.004149377593360996\n",
            "humor: 0.012448132780082987\n",
            "meaningful: 0.004149377593360996\n",
            "message: 0.004149377593360996\n",
            "making: 0.008298755186721992\n",
            "forgettable: 0.004149377593360996\n",
            "disappointing: 0.004149377593360996\n",
            "fifty: 0.008298755186721992\n",
            "shade: 0.008298755186721992\n",
            "grey: 0.008298755186721992\n",
            "cringe-inducing: 0.004149377593360996\n",
            "attempt: 0.004149377593360996\n",
            "romance: 0.004149377593360996\n",
            "poorly: 0.008298755186721992\n",
            "written: 0.004149377593360996\n",
            "dialogue: 0.012448132780082987\n",
            "unconvincing: 0.004149377593360996\n",
            "chemistry: 0.008298755186721992\n",
            "lead: 0.004149377593360996\n",
            "make: 0.004149377593360996\n",
            "awkward: 0.004149377593360996\n",
            "unfulfilling: 0.004149377593360996\n",
            "cinematic: 0.012448132780082987\n",
            "experience: 0.008298755186721992\n",
            "jack: 0.004149377593360996\n",
            "jill: 0.004149377593360996\n",
            "unbearable: 0.004149377593360996\n",
            "comedy: 0.008298755186721992\n",
            "relies: 0.004149377593360996\n",
            "stale: 0.004149377593360996\n",
            "painfully: 0.004149377593360996\n",
            "unfunny: 0.004149377593360996\n",
            "portrayal: 0.008298755186721992\n",
            "adam: 0.004149377593360996\n",
            "sandler: 0.004149377593360996\n",
            "dual: 0.004149377593360996\n",
            "role: 0.004149377593360996\n",
            "prime: 0.004149377593360996\n",
            "example: 0.004149377593360996\n",
            "lazy: 0.004149377593360996\n",
            "filmmaking: 0.004149377593360996\n",
            "superman: 0.004149377593360996\n",
            "iv: 0.004149377593360996\n",
            "colossal: 0.004149377593360996\n",
            "disappointment: 0.004149377593360996\n",
            "marred: 0.004149377593360996\n",
            "low: 0.004149377593360996\n",
            "budget: 0.004149377593360996\n",
            "laughable: 0.008298755186721992\n",
            "conceived: 0.004149377593360996\n",
            "story: 0.004149377593360996\n",
            "even: 0.004149377593360996\n",
            "christopher: 0.004149377593360996\n",
            "reeve: 0.004149377593360996\n",
            "charm: 0.008298755186721992\n",
            "ca: 0.004149377593360996\n",
            "n't: 0.004149377593360996\n",
            "save: 0.004149377593360996\n",
            "mess: 0.008298755186721992\n",
            "cat: 0.004149377593360996\n",
            "hat: 0.004149377593360996\n",
            "chaotic: 0.004149377593360996\n",
            "misguided: 0.004149377593360996\n",
            "dr: 0.004149377593360996\n",
            "seuss: 0.004149377593360996\n",
            "classic: 0.008298755186721992\n",
            "sacrifice: 0.004149377593360996\n",
            "simplicity: 0.004149377593360996\n",
            "source: 0.004149377593360996\n",
            "material: 0.004149377593360996\n",
            "crude: 0.004149377593360996\n",
            "lackluster: 0.004149377593360996\n",
            "narrative: 0.004149377593360996\n",
            "room: 0.004149377593360996\n",
            "widely: 0.008298755186721992\n",
            "regarded: 0.004149377593360996\n",
            "one: 0.004149377593360996\n",
            "worst: 0.004149377593360996\n",
            "ever: 0.004149377593360996\n",
            "made: 0.004149377593360996\n",
            "disjointed: 0.004149377593360996\n",
            "stilted: 0.004149377593360996\n",
            "bizarre: 0.004149377593360996\n",
            "turned: 0.004149377593360996\n",
            "cult: 0.004149377593360996\n",
            "right: 0.004149377593360996\n",
            "reason: 0.004149377593360996\n",
            "battlefield: 0.004149377593360996\n",
            "earth: 0.004149377593360996\n",
            "sci-fi: 0.004149377593360996\n",
            "hammy: 0.004149377593360996\n",
            "remained: 0.004149377593360996\n",
            "buried: 0.004149377593360996\n",
            "annals: 0.004149377593360996\n",
            "history: 0.004149377593360996\n",
            "gigli: 0.004149377593360996\n",
            "train: 0.004149377593360996\n",
            "wreck: 0.004149377593360996\n",
            "romantic: 0.004149377593360996\n",
            "pairing: 0.004149377593360996\n",
            "ben: 0.004149377593360996\n",
            "affleck: 0.004149377593360996\n",
            "jennifer: 0.004149377593360996\n",
            "lopez: 0.004149377593360996\n",
            "devoid: 0.004149377593360996\n",
            "cringeworthy: 0.004149377593360996\n",
            "embarrassing: 0.004149377593360996\n",
            "misstep: 0.004149377593360996\n",
            "career: 0.004149377593360996\n",
            "divided: 0.004149377593360996\n",
            "enthusiast: 0.004149377593360996\n",
            "critic: 0.004149377593360996\n",
            "share: 0.004149377593360996\n",
            "devoted: 0.004149377593360996\n",
            "also: 0.004149377593360996\n",
            "face: 0.004149377593360996\n",
            "substantial: 0.004149377593360996\n",
            "criticism: 0.004149377593360996\n",
            "content: 0.004149377593360996\n",
            "execution: 0.004149377593360996\n",
            "leave: 0.008298755186721992\n",
            "much: 0.004149377593360996\n",
            "desired: 0.004149377593360996\n",
            "polarizing: 0.004149377593360996\n",
            "viewer: 0.008298755186721992\n",
            "reaction: 0.004149377593360996\n",
            "may: 0.012448132780082987\n",
            "vary: 0.004149377593360996\n",
            "evokes: 0.004149377593360996\n",
            "strong: 0.004149377593360996\n",
            "opinion: 0.004149377593360996\n",
            "end: 0.004149377593360996\n",
            "spectrum: 0.004149377593360996\n",
            "relationship: 0.004149377593360996\n",
            "theme: 0.004149377593360996\n",
            "many: 0.004149377593360996\n",
            "uncomfortable: 0.004149377593360996\n",
            "test: 0.004149377593360996\n",
            "limit: 0.004149377593360996\n",
            "find: 0.004149377593360996\n",
            "acceptable: 0.004149377593360996\n",
            "mainstream: 0.004149377593360996\n",
            "cinema: 0.004149377593360996\n",
            "idea: 0.004149377593360996\n",
            "keeping: 0.004149377593360996\n",
            "open: 0.004149377593360996\n",
            "mind: 0.004149377593360996\n",
            "essential: 0.004149377593360996\n",
            "film-watching: 0.004149377593360996\n",
            "particular: 0.004149377593360996\n",
            "might: 0.004149377593360996\n",
            "push: 0.004149377593360996\n",
            "openness: 0.004149377593360996\n",
            "boundary: 0.004149377593360996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 5: Uni-gram probability for the following test movie review"
      ],
      "metadata": {
        "id": "pjr3pU0FUPpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_review_probability_unigram(review, unigram_count, vocabulary_size):\n",
        "    probability = 1.0  # Initialize the probability\n",
        "    k = 0.5  # Laplace smoothing - Add one smoothing\n",
        "\n",
        "    total_words = sum(unigram_count.values())\n",
        "    print(f'Total Words: {total_words}')\n",
        "    print(f'Unique Words: {vocabulary_size}')\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Calculate the probability by multiplying individual unigram probabilities with Laplace smoothing\n",
        "    for word in review:  # Use the length of the review\n",
        "        # Print to check values\n",
        "        print(f'Initial P({word}): {unigram_count[word]}')\n",
        "\n",
        "        # Apply Laplace smoothing by adding k to both the numerator and denominator\n",
        "        probability *= (unigram_count[word] + k) / (k * vocabulary_size + total_words)\n",
        "\n",
        "        # Print to check values\n",
        "        print(f'Laplace P({word}): {(unigram_count[word] + k) / (k * vocabulary_size + total_words)}')\n",
        "\n",
        "        print()\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "eHO52YR3URyB"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probability for the test review with respect to positive and negative datasets\n",
        "positive_review_probability_unigram = calculate_review_probability_unigram(preprocessed_test_review, positive_unigram_count, vocabulary_size_positive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cs3eXqEUcyD",
        "outputId": "f33fb3df-c4ab-4580-b055-3669c8b421cc"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 269\n",
            "Unique Words: 229\n",
            "\n",
            "Initial P(clear): 1\n",
            "Laplace P(clear): 0.003911342894393742\n",
            "\n",
            "Initial P(movie): 1\n",
            "Laplace P(movie): 0.003911342894393742\n",
            "\n",
            "Initial P(enthusiast): 1\n",
            "Laplace P(enthusiast): 0.003911342894393742\n",
            "\n",
            "Initial P(critic): 1\n",
            "Laplace P(critic): 0.003911342894393742\n",
            "\n",
            "Initial P(may): 1\n",
            "Laplace P(may): 0.003911342894393742\n",
            "\n",
            "Initial P(everyone): 1\n",
            "Laplace P(everyone): 0.003911342894393742\n",
            "\n",
            "Initial P(taste): 0\n",
            "Laplace P(taste): 0.001303780964797914\n",
            "\n",
            "Initial P(worth): 0\n",
            "Laplace P(worth): 0.001303780964797914\n",
            "\n",
            "Initial P(watching): 0\n",
            "Laplace P(watching): 0.001303780964797914\n",
            "\n",
            "Initial P(open): 1\n",
            "Laplace P(open): 0.003911342894393742\n",
            "\n",
            "Initial P(mind): 2\n",
            "Laplace P(mind): 0.00651890482398957\n",
            "\n",
            "Initial P(form): 0\n",
            "Laplace P(form): 0.001303780964797914\n",
            "\n",
            "Initial P(opinion): 1\n",
            "Laplace P(opinion): 0.003911342894393742\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probability for the test review with respect to positive and negative datasets\n",
        "negative_review_probability_unigram = calculate_review_probability_unigram(preprocessed_test_review, negative_unigram_count, vocabulary_size_positive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIguIke7aBBH",
        "outputId": "689e400d-fb11-4044-a246-c425be0bf182"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 241\n",
            "Unique Words: 229\n",
            "\n",
            "Initial P(clear): 0\n",
            "Laplace P(clear): 0.0014064697609001407\n",
            "\n",
            "Initial P(movie): 4\n",
            "Laplace P(movie): 0.012658227848101266\n",
            "\n",
            "Initial P(enthusiast): 1\n",
            "Laplace P(enthusiast): 0.004219409282700422\n",
            "\n",
            "Initial P(critic): 1\n",
            "Laplace P(critic): 0.004219409282700422\n",
            "\n",
            "Initial P(may): 3\n",
            "Laplace P(may): 0.009845288326300985\n",
            "\n",
            "Initial P(everyone): 0\n",
            "Laplace P(everyone): 0.0014064697609001407\n",
            "\n",
            "Initial P(taste): 0\n",
            "Laplace P(taste): 0.0014064697609001407\n",
            "\n",
            "Initial P(worth): 0\n",
            "Laplace P(worth): 0.0014064697609001407\n",
            "\n",
            "Initial P(watching): 0\n",
            "Laplace P(watching): 0.0014064697609001407\n",
            "\n",
            "Initial P(open): 1\n",
            "Laplace P(open): 0.004219409282700422\n",
            "\n",
            "Initial P(mind): 1\n",
            "Laplace P(mind): 0.004219409282700422\n",
            "\n",
            "Initial P(form): 0\n",
            "Laplace P(form): 0.0014064697609001407\n",
            "\n",
            "Initial P(opinion): 1\n",
            "Laplace P(opinion): 0.004219409282700422\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 6: Predict the category of the test movie review using Uni-Gram model"
      ],
      "metadata": {
        "id": "vjBrfnahaiQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the probabilities\n",
        "print(\"Positive Review Probability:\", positive_review_probability_unigram)\n",
        "print(\"Negative Review Probability:\", negative_review_probability_unigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxP955HZZ4HQ",
        "outputId": "d6abd253-d510-415b-fae9-2c33de72e489"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Review Probability: 1.0318113735731843e-33\n",
            "Negative Review Probability: 1.290155367564122e-33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the probabilities and make a classification decision based on the task\n",
        "if positive_review_probability_unigram > negative_review_probability_unigram:\n",
        "    print(\"Predicted: Positive\")\n",
        "else:\n",
        "    print(\"Predicted: Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI2yq4Twb8P3",
        "outputId": "9ead6848-dd03-4405-f4be-18db10feb03b"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Unigram model, according to the K value in add-k smoothing, model prediction changes.\n",
        "\n",
        "When K=0.5, model prediction is negative (that means model says most of words are coincide with Negative review)\n",
        "\n",
        "Positive Review Probability: 1.1624207195736878e-33\n",
        "\n",
        "Negative Review Probability: 4.075676951702222e-33\n",
        "\n",
        "\n",
        "\n",
        "But When K=0.1, model prediction is positive (that means model says most of words are coincide with positive review)\n",
        "\n",
        "Positive Review Probability: 4.1578896371040005e-36\n",
        "\n",
        "Negative Review Probability: 2.5647538549341524e-36\n"
      ],
      "metadata": {
        "id": "aFbqAYTpxfL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 7: Perplexity"
      ],
      "metadata": {
        "id": "9DPBGLrrzQIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have explained the concept of perplexity in the pdf document which I have submitted for Part A answer"
      ],
      "metadata": {
        "id": "D1K1-NkmzfEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_unigram_perplexity(review, unigram_count, vocabulary_size, k=0.5):\n",
        "    probability = 1.0\n",
        "    word_count = sum(unigram_count.values())\n",
        "\n",
        "    for i in range(1, len(review)):  # Start from 1 to skip <S>\n",
        "        current_word = review[i]\n",
        "\n",
        "        # Apply Laplace smoothing by adding k to both the numerator and denominator\n",
        "        current_word_count = unigram_count[current_word]\n",
        "\n",
        "        # Update perplexity\n",
        "        probability *= (current_word_count + k) / (k * vocabulary_size + word_count)\n",
        "\n",
        "    # Compute perplexity as the inverse probability normalized by the word count\n",
        "    perplexity = math.pow(1 / probability, 1 / word_count)\n",
        "\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "9gqIqlK3zqVg"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for the positive and negative test reviews\n",
        "perplexity_positive = calculate_unigram_perplexity(preprocessed_test_review, positive_unigram_count, vocabulary_size_positive)\n",
        "perplexity_negative = calculate_unigram_perplexity(preprocessed_test_review, negative_unigram_count, vocabulary_size_negative)\n",
        "\n",
        "print(f\"Perplexity for positive test review: {perplexity_positive}\")\n",
        "print(f\"Perplexity for negative test review: {perplexity_negative}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfNCMCUw0NJW",
        "outputId": "021f6ee9-50d2-46c8-a252-c2899286336f"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for positive test review: 1.299198647475049\n",
            "Perplexity for negative test review: 1.3292576489765537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimizing Perplexity is same as Maximizing probability of the sentence."
      ],
      "metadata": {
        "id": "1SMfK89k7Iq8"
      }
    }
  ]
}